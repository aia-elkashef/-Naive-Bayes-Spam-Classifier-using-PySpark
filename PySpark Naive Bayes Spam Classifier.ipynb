{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian_Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset can be downloaded from [UCI Machine Learning Respository](https://archive.ics.uci.edu/ml/machine-learning-databases/00228/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate a spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SpamClassifier').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|class|                text|\n",
      "+-----+--------------------+\n",
      "|  ham|Go until jurong p...|\n",
      "|  ham|Ok lar... Joking ...|\n",
      "| spam|Free entry in 2 a...|\n",
      "|  ham|U dun say so earl...|\n",
      "|  ham|Nah I don't think...|\n",
      "| spam|FreeMsg Hey there...|\n",
      "|  ham|Even my brother i...|\n",
      "|  ham|As per your reque...|\n",
      "| spam|WINNER!! As a val...|\n",
      "| spam|Had your mobile 1...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data and rename column\n",
    "df = spark.read.option(\"header\", \"false\") \\\n",
    "    .option(\"delimiter\", \"\\t\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"SMSSpamCollection.txt\") \\\n",
    "    .withColumnRenamed(\"_c0\", \"class\") \\\n",
    "    .withColumnRenamed(\"_c1\", \"text\")\n",
    "\n",
    "df.limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " RegexTokenizer_d4ae0ba9e0da\n",
      "\n",
      " CountVectorizer_ce863a6732fe\n",
      "\n",
      " StringIndexer_3d2ef229b0c5\n",
      "\n",
      " VectorAssembler_330cb9a0ff40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stages = []\n",
    "# 1. clean data and tokenize sentences using RegexTokenizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"tokens\", pattern=\"\\\\W+\")\n",
    "stages += [regexTokenizer]\n",
    "\n",
    "# 2. CountVectorize the data\n",
    "cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"token_features\", minDF=2.0)#, vocabSize=3, minDF=2.0\n",
    "stages += [cv]\n",
    "\n",
    "# 3. Convert the labels to numerical values using binariser\n",
    "indexer = StringIndexer(inputCol=\"class\", outputCol=\"label\")\n",
    "stages += [indexer]\n",
    "\n",
    "# 4. Vectorise features using vectorassembler\n",
    "vecAssembler = VectorAssembler(inputCols=['token_features'], outputCol=\"features\")\n",
    "stages += [vecAssembler]\n",
    "\n",
    "[print('\\n', stage) for stage in stages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=stages)\n",
    "data = pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = data.randomSplit([0.7, 0.3], seed = 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "model = nb.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------------------------------------------+\n",
      "|label|prediction|probability                               |\n",
      "+-----+----------+------------------------------------------+\n",
      "|0.0  |0.0       |[0.9999996176179956,3.823820044882337E-7] |\n",
      "|0.0  |0.0       |[0.9972054995602091,0.002794500439790882] |\n",
      "|0.0  |0.0       |[0.9999999999978098,2.190326444063966E-12]|\n",
      "|0.0  |0.0       |[0.9999999999999538,4.607804951342392E-14]|\n",
      "|0.0  |0.0       |[0.999999999880886,1.1911406870203127E-10]|\n",
      "|0.0  |0.0       |[0.999688852925206,3.1114707479388615E-4] |\n",
      "|0.0  |0.0       |[0.9999999098737272,9.012627286140461E-8] |\n",
      "|0.0  |0.0       |[0.9999950690131734,4.930986826665776E-6] |\n",
      "|0.0  |0.0       |[0.9999795625725587,2.043742744135259E-5] |\n",
      "|0.0  |0.0       |[0.9999063364041348,9.366359586510845E-5] |\n",
      "+-----+----------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test)\n",
    "# Select results to view\n",
    "predictions.limit(10).select(\"label\", \"prediction\", \"probability\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC:  0.972052252090383\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print (\"Test Area Under ROC: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use <b>MulticlassClassificationEvaluator</b> to calculate the <b>f1_score</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score is:  0.9894791154468984\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "eval_f1 = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "\n",
    "\n",
    "\n",
    "f1score = eval_f1.evaluate(predictions)\n",
    "print (\"f1_score is: \", f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
